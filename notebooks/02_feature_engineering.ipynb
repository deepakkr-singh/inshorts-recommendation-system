{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Pipeline\n",
    "\n",
    "**Inshorts Assignment - Feature Engineering for Recommendation System**\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook builds all features needed for content-based and collaborative filtering approaches.\n",
    "\n",
    "**Output:** All engineered features saved to `data/features/` for use in recommendation algorithms.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Feature Engineering\n",
    "\n",
    "Build all features needed for both content-based and collaborative filtering approaches.\n",
    "\n",
    "**Section Overview:**\n",
    "- **1.1** Load Processed Data\n",
    "- **1.2** Category Features (TF-IDF) - 50% weight\n",
    "- **1.3** Language Preference - 15% weight  \n",
    "- **1.4** NewsType Preference - 10% weight\n",
    "- **1.5** Geographic Location - 10% weight\n",
    "- **1.6** Collaborative Filtering Features (user-user similarity)\n",
    "- **1.7** Save All Features to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_b/f8jj0h8j2nl4j1fggf33t1xm0000gn/T/ipykernel_7879/2212550761.py:8: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  events = pd.read_csv(data_path / 'events.csv', dtype={'deviceId': str, 'hashId': str})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10,400 devices\n",
      "Loaded 3,544,161 events\n",
      "Loaded 8,170 training articles\n",
      "Loaded 970 testing articles\n",
      "\n",
      "Active users: 8,977\n",
      "\n",
      "ðŸ”’ Train split loaded: 2,831,554 events (79.9%)\n",
      "   Will be used for collaborative filtering features to prevent data leakage\n"
     ]
    }
   ],
   "source": [
    "project_root = Path.cwd().parent\n",
    "data_path = project_root / 'data' / 'processed'\n",
    "\n",
    "features_path = project_root / 'data' / 'features'\n",
    "features_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "devices = pd.read_csv(data_path / 'devices.csv', dtype={'deviceid': str})\n",
    "events = pd.read_csv(data_path / 'events.csv', dtype={'deviceId': str, 'hashId': str})\n",
    "training_content = pd.read_csv(data_path / 'training_content.csv', dtype={'hashid': str})\n",
    "testing_content = pd.read_csv(data_path / 'testing_content.csv', dtype={'hashid': str})\n",
    "\n",
    "# CRITICAL: Load train_split for collaborative filtering (avoid data leakage)\n",
    "# - For content-based features: Use all events (user preferences)\n",
    "# - For collaborative filtering: Use ONLY train_split (80% of events)\n",
    "train_split = pd.read_csv(data_path / 'train_split.csv', dtype={'deviceId': str, 'hashId': str})\n",
    "\n",
    "print(f\"Loaded {len(devices):,} devices\")\n",
    "print(f\"Loaded {len(events):,} events\")\n",
    "print(f\"Loaded {len(training_content):,} training articles\")\n",
    "print(f\"Loaded {len(testing_content):,} testing articles\")\n",
    "print(f\"\\nActive users: {events['deviceId'].nunique():,}\")\n",
    "print(f\"\\nðŸ”’ Train split loaded: {len(train_split):,} events ({len(train_split)/len(events)*100:.1f}%)\")\n",
    "print(f\"   Will be used for collaborative filtering features to prevent data leakage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Category Features (TF-IDF)\n",
    "\n",
    "**Assignment Requirement:** User's reading history + expressed interests â†’ Category matching (50% weight)\n",
    "\n",
    "**Method:** TF-IDF vectorization of article categories + user reading profiles\n",
    "\n",
    "**Why TF-IDF:** Captures both frequency (user reads \"politics\" often) and specificity (rare topics like \"cryptocurrency\" get higher weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Article Feature Engineering (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Shapes:\n",
      "   Training: (8170, 31)\n",
      "   Testing:  (970, 31)\n",
      "\n",
      "Vocabulary size: 31\n",
      "\n",
      "Feature names (first 10):\n",
      "   ['ashes_2023', 'automobile', 'bank_frauds', 'business', 'coronavirus', 'cryptocurrency', 'education', 'entertainment', 'fashion', 'hatke']\n",
      "\n",
      "Matrix Density:\n",
      "   Training: 3.96% (non-zero: 10039)\n",
      "   Testing:  3.55% (non-zero: 1066)\n"
     ]
    }
   ],
   "source": [
    "def prepare_category_text(df):\n",
    "    \"\"\"Extract categories column as-is (comma-separated strings)\"\"\"\n",
    "    text_features = []\n",
    "    for _, row in df.iterrows():\n",
    "        cats = str(row['categories']).strip() if pd.notna(row['categories']) else \"\"\n",
    "        text_features.append(cats)\n",
    "    return text_features\n",
    "\n",
    "# Create a named function for tokenizer to allow pickling\n",
    "def comma_tokenizer(x):\n",
    "    return [\n",
    "        part.strip().lower()\n",
    "        for part in x.split(',')\n",
    "        if part.strip() and ' ' not in part.strip()\n",
    "    ]\n",
    "\n",
    "# Prepare category text\n",
    "training_texts = prepare_category_text(training_content)\n",
    "testing_texts = prepare_category_text(testing_content)\n",
    "\n",
    "# Create TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=comma_tokenizer,  # Use named function\n",
    "    lowercase=True,          # Convert to lowercase\n",
    "    min_df=2,                # Keep categories appearing in â‰¥2 articles\n",
    "    max_features=None,       # Don't limit features (let min_df control)\n",
    "    token_pattern=None       # Required when using custom tokenizer\n",
    ")\n",
    "\n",
    "# Fit on training, transform both\n",
    "training_tfidf = tfidf_vectorizer.fit_transform(training_texts)\n",
    "testing_tfidf = tfidf_vectorizer.transform(testing_texts)\n",
    "\n",
    "# Verify results\n",
    "print(f\"TF-IDF Shapes:\")\n",
    "print(f\"   Training: {training_tfidf.shape}\")  # Should be (8170, 38)\n",
    "print(f\"   Testing:  {testing_tfidf.shape}\")   # Should be (970, 38)\n",
    "print(f\"\\nVocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")  # Should be 38\n",
    "print(f\"\\nFeature names (first 10):\")\n",
    "print(f\"   {list(tfidf_vectorizer.get_feature_names_out())[:10]}\")\n",
    "\n",
    "# Check density\n",
    "train_density = (training_tfidf.nnz / (training_tfidf.shape[0] * training_tfidf.shape[1])) * 100\n",
    "test_density = (testing_tfidf.nnz / (testing_tfidf.shape[0] * testing_tfidf.shape[1])) * 100\n",
    "print(f\"\\nMatrix Density:\")\n",
    "print(f\"   Training: {train_density:.2f}% (non-zero: {training_tfidf.nnz})\")\n",
    "print(f\"   Testing:  {test_density:.2f}% (non-zero: {testing_tfidf.nnz})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 User Feature Engineering (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built category profiles for 8,689 users\n",
      "User category TF-IDF shape: (8689, 31)\n",
      "Average interactions per user: 211.4\n"
     ]
    }
   ],
   "source": [
    "def build_user_profiles_optimized(merged_df):\n",
    "    # Extract categories and create weighted category lists\n",
    "    valid_data = merged_df[merged_df['categories'].notna()].copy()\n",
    "    \n",
    "    valid_data['category_list'] = valid_data['categories'].str.split(',').apply(\n",
    "        lambda x: [c.strip() for c in x if c.strip()]\n",
    "    )\n",
    "    \n",
    "    # Repeat categories by engagement weight\n",
    "    valid_data['weighted_categories'] = valid_data.apply(\n",
    "        lambda row: row['category_list'] * int(row['weight']),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Group by user and concatenate all weighted categories\n",
    "    user_profiles = valid_data.groupby('deviceId').agg({\n",
    "        'weighted_categories': lambda x: ' '.join([cat for cats in x for cat in cats]),\n",
    "        'deviceId': 'count'  \n",
    "    }).rename(columns={'deviceId': 'num_interactions'}).reset_index()\n",
    "    \n",
    "    user_profiles.columns = ['deviceId', 'category_text', 'num_interactions']\n",
    "    \n",
    "    return user_profiles\n",
    "\n",
    "# Merge events with article categories\n",
    "merged = events.merge(\n",
    "    training_content[['hashid', 'categories', 'newsType']],\n",
    "    left_on='hashId',\n",
    "    right_on='hashid',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Define engagement weights\n",
    "event_weights = {\n",
    "    'TimeSpent-Front': 1.0,\n",
    "    'TimeSpent-Back': 2.0,\n",
    "    'News Bookmarked': 3.0,\n",
    "    'News Shared': 5.0\n",
    "}\n",
    "\n",
    "# Apply weights and build user profiles\n",
    "merged['weight'] = merged['event_type'].map(event_weights).fillna(1.0)\n",
    "user_profiles = build_user_profiles_optimized(merged)\n",
    "\n",
    "# Transform user profiles to TF-IDF vectors\n",
    "user_category_tfidf = tfidf_vectorizer.transform(user_profiles['category_text'])\n",
    "\n",
    "print(f\"\\nBuilt category profiles for {len(user_profiles):,} users\")\n",
    "print(f\"User category TF-IDF shape: {user_category_tfidf.shape}\")\n",
    "print(f\"Average interactions per user: {user_profiles['num_interactions'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Article Popularity Score\n",
    "\n",
    "**Assignment Requirement:** Popularity of articles â†’ Quality signal (15% weight)\n",
    "\n",
    "**Method:** Combined metric of unique users + total engagement, log-normalized\n",
    "\n",
    "**Formula:** `popularity = log(unique_users) Ã— 0.7 + log(total_events) Ã— 0.3`\n",
    "\n",
    "**Why:** Balances virality (many users) with depth (repeated engagement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed popularity for 14,622 articles\n",
      "Popularity range: 0.083 to 1.000\n"
     ]
    }
   ],
   "source": [
    "# Calculate article popularity scores\n",
    "popularity_stats = events.groupby('hashId').agg({\n",
    "    'deviceId': 'nunique',     # Unique users (virality)\n",
    "    'event_type': 'count'       # Total events (depth)\n",
    "}).rename(columns={\n",
    "    'deviceId': 'unique_users',\n",
    "    'event_type': 'total_events'\n",
    "})\n",
    "\n",
    "# Combined metric: log(unique_users) Ã— 0.7 + log(total_events) Ã— 0.3\n",
    "popularity_stats['popularity_score'] = (\n",
    "    np.log1p(popularity_stats['unique_users']) * 0.7 +\n",
    "    np.log1p(popularity_stats['total_events']) * 0.3\n",
    ")\n",
    "\n",
    "# Normalize to 0-1 range\n",
    "max_pop = popularity_stats['popularity_score'].max()\n",
    "popularity_stats['popularity_normalized'] = popularity_stats['popularity_score'] / max_pop\n",
    "\n",
    "article_popularity = popularity_stats['popularity_normalized'].to_dict()\n",
    "\n",
    "print(f\"Computed popularity for {len(article_popularity):,} articles\")\n",
    "print(f\"Popularity range: {popularity_stats['popularity_normalized'].min():.3f} to {popularity_stats['popularity_normalized'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Article Popularity Score\n",
    "\n",
    "**Why:** Balances virality (many users) with depth (repeated engagement)\n",
    "\n",
    "**Assignment Requirement:** Popularity of articles â†’ Quality signal (15% weight)\n",
    "\n",
    "**Formula:** `popularity = log(unique_users) Ã— 0.7 + log(total_events) Ã— 0.3`\n",
    "\n",
    "**Method:** Combined metric of unique users + total engagement, log-normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles: 9,140\n",
      "Training articles: 8,170\n",
      "Testing articles: 970\n",
      "\n",
      "Preserved columns: ['hashid', 'categories', 'newsLanguage', 'newsType', 'language', 'newstype', 'popularity', 'is_test']\n",
      "\n",
      "Language distribution:\n",
      "language\n",
      "english     6575\n",
      "hindi       2216\n",
      "telugu        94\n",
      "kannada       76\n",
      "gujarati      64\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Select essential columns from both datasets\n",
    "training_cols = ['hashid', 'categories', 'newsLanguage', 'newsType']\n",
    "testing_cols = ['hashid', 'categories', 'newsLanguage', 'newsType']\n",
    "\n",
    "# Combine training + testing articles\n",
    "all_content = pd.concat([\n",
    "    training_content[training_cols],\n",
    "    testing_content[testing_cols]\n",
    "], ignore_index=True)\n",
    "\n",
    "# Rename for clarity (keep originals for compatibility)\n",
    "all_content['language'] = all_content['newsLanguage']\n",
    "all_content['newstype'] = all_content['newsType']\n",
    "\n",
    "# Add popularity scores (0.0 for testing articles with no history)\n",
    "all_content['popularity'] = all_content['hashid'].map(article_popularity).fillna(0.0)\n",
    "\n",
    "# Add test flag\n",
    "all_content['is_test'] = all_content['hashid'].isin(testing_content['hashid'])\n",
    "\n",
    "print(f\"Total articles: {len(all_content):,}\")\n",
    "print(f\"Training articles: {(~all_content['is_test']).sum():,}\")\n",
    "print(f\"Testing articles: {all_content['is_test'].sum():,}\")\n",
    "print(f\"\\nPreserved columns: {list(all_content.columns)}\")\n",
    "print(f\"\\nLanguage distribution:\")\n",
    "print(all_content['language'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Language Preference Encoding\n",
    "\n",
    "**Solution:** Behavioral inference from reading history - what they actually read reveals preference\n",
    "\n",
    "**Assignment Requirement:** User's expressed interests â†’ Language preference matching (15% weight)\n",
    "\n",
    "**Challenge:** Only 7.87% users have explicit `language_preference` in devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 User Language Encoding\n",
    "\n",
    "**Strategy: Behavioral Inference (not explicit preference)**\n",
    "- **Problem**: `devices.csv['language_preference']` only 7.87% coverage (684/8,689 users)\n",
    "- **Solution**: Infer from reading history (which articles they actually read)\n",
    "- **Method**: Mode (most-read language) from user's article consumption\n",
    "- **Result**: 99.99% coverage (only 1 user needs default 'english')\n",
    "\n",
    "**Why this works:**\n",
    "- All users have reading history (by definition in system)\n",
    "- Reading behavior reveals true preference (Hindi readers â†’ Hindi articles)\n",
    "- Avoids bias (no arbitrary English default for 92%)\n",
    "- Behavioral truth: If user reads Hindi, recommend Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring user language preferences from reading history...\n",
      "   Using vectorized operations (much faster!)...\n",
      "\n",
      "User Language Coverage: 8689/8689 (100%)\n",
      "\n",
      "User Language Distribution:\n",
      "inferred_language\n",
      "english    8677\n",
      "hindi        12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "User language inference complete (vectorized optimization)!\n",
      "   Column added: 'final_language' (8689 users)\n"
     ]
    }
   ],
   "source": [
    "print(\"Inferring user language preferences from reading history...\")\n",
    "print(f\"   Using vectorized operations (much faster!)...\")\n",
    "\n",
    "# Step 1: Merge events with article languages\n",
    "events_with_lang = events.merge(\n",
    "    all_content[['hashid', 'newsLanguage']], \n",
    "    left_on='hashId', \n",
    "    right_on='hashid', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 2: Group by user and compute mode (ignoring NaN)\n",
    "user_lang_mode = events_with_lang.groupby('deviceId')['newsLanguage'].agg(\n",
    "    lambda x: x.dropna().mode()[0] if len(x.dropna().mode()) > 0 else 'english'\n",
    ").rename('inferred_language')\n",
    "\n",
    "# Step 3: Add to user_profiles\n",
    "user_profiles = user_profiles.merge(user_lang_mode, on='deviceId', how='left')\n",
    "user_profiles['inferred_language'] = user_profiles['inferred_language'].fillna('english')\n",
    "user_profiles['final_language'] = user_profiles['inferred_language']\n",
    "\n",
    "# Check coverage\n",
    "inferred_lang_users = user_profiles['inferred_language'].notna().sum()\n",
    "\n",
    "print(f\"\\nUser Language Coverage: {inferred_lang_users}/{len(user_profiles)} (100%)\")\n",
    "print(f\"\\nUser Language Distribution:\")\n",
    "print(user_profiles['inferred_language'].value_counts())\n",
    "print(f\"\\nUser language inference complete (vectorized optimization)!\")\n",
    "print(f\"   Column added: 'final_language' ({len(user_profiles)} users)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Article Language Encoding\n",
    "\n",
    "**Now we can fill article languages for matching**\n",
    "- Fill 112 missing article languages with 'english' (71.94% mode)\n",
    "- This happens AFTER user inference (prevents bias)\n",
    "- One-hot encode for efficient matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article languages filled: 112 NaN â†’ 'english'\n",
      "Language features added: ['lang_ANI', 'lang_Twitter', 'lang_english', 'lang_gujarati', 'lang_hindi', 'lang_kannada', 'lang_telugu', 'lang_à¤­à¤¾à¤·à¤¾']\n",
      "\n",
      "Language distribution:\n",
      "lang_english     6687\n",
      "lang_hindi       2216\n",
      "lang_telugu        94\n",
      "lang_kannada       76\n",
      "lang_gujarati      64\n",
      "lang_ANI            1\n",
      "lang_Twitter        1\n",
      "lang_à¤­à¤¾à¤·à¤¾           1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill missing article languages (AFTER user inference to prevent bias)\n",
    "all_content['newsLanguage'] = all_content['newsLanguage'].fillna('english')\n",
    "\n",
    "# One-hot encode newsLanguage for efficient matching\n",
    "language_dummies = pd.get_dummies(all_content['newsLanguage'], prefix='lang')\n",
    "\n",
    "# Add to all_content\n",
    "all_content = pd.concat([all_content, language_dummies], axis=1)\n",
    "\n",
    "print(f\"Article languages filled: 112 NaN â†’ 'english'\")\n",
    "print(f\"Language features added: {list(language_dummies.columns)}\")\n",
    "print(f\"\\nLanguage distribution:\")\n",
    "print(language_dummies.sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 NewsType Preference Encoding\n",
    "\n",
    "**Assignment Requirement:** User's reading history â†’ Content type preference (10% weight)\n",
    "\n",
    "**Types:** NEWS (standard text+image), VIDEO_NEWS, PHOTO, INFOGRAPHIC\n",
    "\n",
    "**Solution:** Infer from reading behavior - predict preferred content format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 User NewsType Encoding\n",
    "\n",
    "**Strategy: Behavioral Inference (same as language)**\n",
    "- **Problem**: No explicit user newsType preference in data\n",
    "- **Solution**: Infer from reading history (what types they actually read)\n",
    "- **Method**: Mode (most-read type) from user's article consumption\n",
    "- **Result**: 100% coverage (all users have reading history)\n",
    "\n",
    "**Why this works:**\n",
    "- All users have reading history (by definition)\n",
    "- Reading behavior reveals preference (video consumers â†’ VIDEO_NEWS)\n",
    "- Most users (98%+) prefer standard NEWS articles\n",
    "- Critical for niche users (video consumers need video content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring user newsType preferences from reading history...\n",
      "   Using vectorized operations (fast!)...\n",
      "\n",
      "User NewsType Coverage: 8689/8689 (100%)\n",
      "\n",
      "User NewsType Distribution:\n",
      "preferred_newsType\n",
      "NEWS                                                                                                                                                               8688\n",
      " \\\"Where is my tax money going!?...Enough is really enough! We need better infrastructure!!\\\" Tagging the Brihanmumbai Municipal Corporation (BMC) in the tweet       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "User newsType inference complete!\n",
      "   Column added: 'preferred_newsType' (8689 users)\n"
     ]
    }
   ],
   "source": [
    "print(\"Inferring user newsType preferences from reading history...\")\n",
    "print(f\"   Using vectorized operations (fast!)...\")\n",
    "\n",
    "# Step 1: Merge events with article types\n",
    "events_with_type = events.merge(\n",
    "    all_content[['hashid', 'newsType']], \n",
    "    left_on='hashId', \n",
    "    right_on='hashid', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 2: Group by user and compute mode (ignoring NaN)\n",
    "user_type_mode = events_with_type.groupby('deviceId')['newsType'].agg(\n",
    "    lambda x: x.dropna().mode()[0] if len(x.dropna().mode()) > 0 else 'NEWS'\n",
    ").rename('preferred_newsType')\n",
    "\n",
    "# Step 3: Add to user_profiles\n",
    "user_profiles = user_profiles.merge(user_type_mode, on='deviceId', how='left')\n",
    "user_profiles['preferred_newsType'] = user_profiles['preferred_newsType'].fillna('NEWS')\n",
    "\n",
    "# Check coverage\n",
    "users_with_type = user_profiles['preferred_newsType'].notna().sum()\n",
    "\n",
    "print(f\"\\nUser NewsType Coverage: {users_with_type}/{len(user_profiles)} (100%)\")\n",
    "print(f\"\\nUser NewsType Distribution:\")\n",
    "print(user_profiles['preferred_newsType'].value_counts())\n",
    "print(f\"\\nUser newsType inference complete!\")\n",
    "print(f\"   Column added: 'preferred_newsType' ({len(user_profiles)} users)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Article NewsType Encoding\n",
    "\n",
    "**Now we can fill article newsType for matching**\n",
    "- Fill 348 missing article newsType with 'NEWS' (93% mode)\n",
    "- This happens AFTER user inference (prevents bias)\n",
    "- Label encode for efficient matching (NEWS=0, VIDEO_NEWS=1, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article newsType filled: 348 NaN â†’ 'NEWS'\n",
      "NewsType encoding mapping:\n",
      "   {'VIDEO_NEWS': 0, 'NEWS': 1, '61efae73573ea241dc329e00': 2, 'national\"': 3, '5f70e895d43821580e6d7053': 4, ' consequential and enjoyable\\\\\" G20 Finance Ministers and Central Bank Governors (FMCBG) meeting. The Indian-American economist also ': 5, 'technology': 6, ' she should reconsider her decision': 7, 'politics': 8, ' it has been six months': 9, ' 2023 à¤•à¥‡ ': 10, 'business': 11, '\\\\\" he said. ': 12, ' actor Shah Rukh Khan said that father bias and excitement will always be there. \\\\\"But [I am] looking forward to a...Zoya Akhtar film actually': 13, 'entertainment': 14, 'politics,national': 15, '\\\\\" Shah Rukh replied. He was replying to the fan as part of \\'Ask SRK\\' session on Twitter. He also answered question about ': 16, ' \\\\\"Where is my tax money going!?...Enough is really enough! We need better infrastructure!!\\\\\" Tagging the Brihanmumbai Municipal Corporation (BMC) in the tweet': 17, 'entertainment,national': 18, ' your pelvic bones show': 19, '61efaefad4382134432b6184': 20, \" you're at it again! Attacking our...name\": 21}\n",
      "\n",
      "NewsType distribution:\n",
      "newsType\n",
      "NEWS                                                                                                                                                               8882\n",
      "VIDEO_NEWS                                                                                                                                                          231\n",
      "politics                                                                                                                                                              4\n",
      "entertainment                                                                                                                                                         2\n",
      "national\"                                                                                                                                                             2\n",
      "technology                                                                                                                                                            2\n",
      " it has been six months                                                                                                                                               2\n",
      "61efaefad4382134432b6184                                                                                                                                              1\n",
      " your pelvic bones show                                                                                                                                               1\n",
      "entertainment,national                                                                                                                                                1\n",
      " \\\"Where is my tax money going!?...Enough is really enough! We need better infrastructure!!\\\" Tagging the Brihanmumbai Municipal Corporation (BMC) in the tweet       1\n",
      "\\\" Shah Rukh replied. He was replying to the fan as part of 'Ask SRK' session on Twitter. He also answered question about                                             1\n",
      "politics,national                                                                                                                                                     1\n",
      "business                                                                                                                                                              1\n",
      " actor Shah Rukh Khan said that father bias and excitement will always be there. \\\"But [I am] looking forward to a...Zoya Akhtar film actually                        1\n",
      "\\\" he said.                                                                                                                                                           1\n",
      " 2023 à¤•à¥‡                                                                                                                                                              1\n",
      " she should reconsider her decision                                                                                                                                   1\n",
      " consequential and enjoyable\\\" G20 Finance Ministers and Central Bank Governors (FMCBG) meeting. The Indian-American economist also                                   1\n",
      "5f70e895d43821580e6d7053                                                                                                                                              1\n",
      "61efae73573ea241dc329e00                                                                                                                                              1\n",
      " you're at it again! Attacking our...name                                                                                                                             1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill missing article newsType (AFTER user inference)\n",
    "all_content['newsType'] = all_content['newsType'].fillna('NEWS')\n",
    "\n",
    "# Label encode newsType for preference matching\n",
    "newstype_mapping = {\n",
    "    newstype: idx for idx, newstype in enumerate(all_content['newsType'].unique())\n",
    "}\n",
    "\n",
    "all_content['newsType_encoded'] = all_content['newsType'].map(newstype_mapping)\n",
    "\n",
    "print(f\"Article newsType filled: 348 NaN â†’ 'NEWS'\")\n",
    "print(f\"NewsType encoding mapping:\")\n",
    "print(f\"   {newstype_mapping}\")\n",
    "print(f\"\\nNewsType distribution:\")\n",
    "print(all_content['newsType'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Geographic Location Encoding\n",
    "\n",
    "**Solution:** Infer article geography from reader patterns + soft scoring for diversity\n",
    "\n",
    "**Assignment Requirement:** Relevance to user's location â†’ Geographic preference (10% weight)\n",
    "\n",
    "**Challenge:** District field only 0.2% complete, city field (lastknownsubadminarea) 91.3% complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Article Location Inference (Reader Geography)\n",
    "\n",
    "**Critical: This MUST happen BEFORE user location imputation**\n",
    "\n",
    "**Why this order matters:**\n",
    "- Similar to language/type: Infer from clean behavioral data FIRST\n",
    "- Article location inference uses users who HAVE city data\n",
    "- If we impute user cities first, it would bias the inference\n",
    "- By inferring article locations FIRST, we preserve true reader geography patterns\n",
    "\n",
    "**Strategy:**\n",
    "- Use `lastknownsubadminarea` (city field) from devices.csv - 91.3% completeness\n",
    "- For each article, analyze which cities read it\n",
    "- Rule: If â‰¥50% of reads come from one city â†’ tag article with that city\n",
    "- Otherwise â†’ tag as \"NATIONAL\" (broad appeal)\n",
    "\n",
    "**Expected Coverage:**\n",
    "- Users with city data: ~684 users (7.9% overlap with events)\n",
    "- Events from these users: ~2-3% of total\n",
    "- Result: ~10% articles get specific city tags, ~90% NATIONAL\n",
    "\n",
    "**Scoring (used in recommender):**\n",
    "- Same city match: 1.0 (perfect local relevance)\n",
    "- NATIONAL or user unknown: 0.7 (neutral - good for all)\n",
    "- Different city: 0.5 (mild penalty for geographic mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring article locations from reader geography...\n",
      "   Critical: This happens BEFORE user location imputation!\n",
      "\n",
      "Users with city data: 9,492 / 10,400 = 91.3%\n",
      "Events from users with city: 104,069 / 3,544,161 = 2.9%\n",
      "\n",
      "Article location inference complete:\n",
      "   Articles with specific city tags: 1,631 (17.8%)\n",
      "   Articles tagged as NATIONAL: 7,521 (82.2%)\n",
      "   Total articles analyzed: 9,152\n",
      "\n",
      "Total articles with location tags: 13,702\n",
      "   (Includes training + testing articles)\n",
      "\n",
      "Top 10 cities by article count:\n",
      "Ahmedabad      234\n",
      "Shimoga        210\n",
      "Mumbai         124\n",
      "Chennai        121\n",
      "Hyderabad      111\n",
      "Noida           77\n",
      "Hoshangabad     68\n",
      "Lucknow         58\n",
      "Delhi           47\n",
      "Bengaluru       44\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Inferring article locations from reader geography...\")\n",
    "print(f\"   Critical: This happens BEFORE user location imputation!\")\n",
    "\n",
    "# Step 1: Get users with city data (from devices.csv)\n",
    "user_metadata = devices[['deviceid', 'lastknownsubadminarea']].copy()\n",
    "user_metadata.columns = ['deviceId', 'user_city']\n",
    "user_metadata = user_metadata[user_metadata['user_city'].notna()]\n",
    "\n",
    "print(f\"\\nUsers with city data: {len(user_metadata):,} / {len(devices):,} = {len(user_metadata)/len(devices)*100:.1f}%\")\n",
    "\n",
    "# Step 2: Merge events with user cities\n",
    "events_with_city = events.merge(user_metadata, on='deviceId', how='inner')\n",
    "print(f\"Events from users with city: {len(events_with_city):,} / {len(events):,} = {len(events_with_city)/len(events)*100:.1f}%\")\n",
    "\n",
    "# Step 3: Calculate city distribution for each article\n",
    "article_city_counts = events_with_city.groupby(['hashId', 'user_city']).size().reset_index(name='reads')\n",
    "article_total_reads = events_with_city.groupby('hashId').size().reset_index(name='total_reads')\n",
    "\n",
    "article_city_counts = article_city_counts.merge(article_total_reads, on='hashId')\n",
    "article_city_counts['city_percentage'] = article_city_counts['reads'] / article_city_counts['total_reads']\n",
    "\n",
    "# Step 4: Infer article locations (50% threshold)\n",
    "article_locations = {}\n",
    "local_threshold = 0.50  # 50% of reads from one city\n",
    "\n",
    "for article in article_city_counts['hashId'].unique():\n",
    "    article_data = article_city_counts[article_city_counts['hashId'] == article]\n",
    "    max_row = article_data.loc[article_data['city_percentage'].idxmax()]\n",
    "    \n",
    "    if max_row['city_percentage'] >= local_threshold:\n",
    "        article_locations[article] = max_row['user_city']\n",
    "    else:\n",
    "        article_locations[article] = 'NATIONAL'\n",
    "\n",
    "# Step 5: Statistics\n",
    "location_stats = pd.Series(article_locations).value_counts()\n",
    "num_local = (location_stats.drop('NATIONAL', errors='ignore')).sum() if 'NATIONAL' in location_stats else location_stats.sum()\n",
    "num_national = location_stats.get('NATIONAL', 0)\n",
    "\n",
    "print(f\"\\nArticle location inference complete:\")\n",
    "print(f\"   Articles with specific city tags: {num_local:,} ({num_local/len(article_locations)*100:.1f}%)\")\n",
    "print(f\"   Articles tagged as NATIONAL: {num_national:,} ({num_national/len(article_locations)*100:.1f}%)\")\n",
    "print(f\"   Total articles analyzed: {len(article_locations):,}\")\n",
    "\n",
    "# Step 6: For articles not in events (testing), tag as NATIONAL\n",
    "all_article_ids = set(all_content['hashid'].tolist())\n",
    "for article_id in all_article_ids:\n",
    "    if article_id not in article_locations:\n",
    "        article_locations[article_id] = 'NATIONAL'\n",
    "\n",
    "print(f\"\\nTotal articles with location tags: {len(article_locations):,}\")\n",
    "print(f\"   (Includes training + testing articles)\")\n",
    "\n",
    "# Show top cities\n",
    "print(f\"\\nTop 10 cities by article count:\")\n",
    "print(location_stats.drop('NATIONAL', errors='ignore').head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 User Location Encoding\n",
    "\n",
    "**Now we can impute user locations (AFTER article inference)**\n",
    "\n",
    "**Strategy:**\n",
    "- For 8,689 active users, check if they have city data in devices.csv\n",
    "- If present â†’ use it\n",
    "- If missing â†’ set as 'UNKNOWN' (not infer)\n",
    "- Result: ~684 users with known city (7.9%), rest UNKNOWN\n",
    "\n",
    "**Why 'UNKNOWN' instead of inferring:**\n",
    "- Only 7.9% users have city data in devices\n",
    "- Inferring from reading would be unreliable (circular logic)\n",
    "- Better to be honest: unknown â†’ neutral score (0.7)\n",
    "- For known users â†’ can boost local content (1.0) or penalize far content (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User location encoding complete:\n",
      "   Users with known city: 684 (7.9%)\n",
      "   Users with UNKNOWN city: 8,005 (92.1%)\n",
      "\n",
      "Top 10 user cities:\n",
      "user_city\n",
      "Mumbai       38\n",
      "Delhi        33\n",
      "Bengaluru    30\n",
      "Kolkata      23\n",
      "Noida        21\n",
      "Patna        19\n",
      "Gurgaon      15\n",
      "Lucknow      14\n",
      "Chennai      14\n",
      "Hyderabad    13\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# User location imputation (AFTER article location inference)\n",
    "user_city_map = devices[['deviceid', 'lastknownsubadminarea']].copy()\n",
    "user_city_map.columns = ['deviceId', 'user_city']\n",
    "\n",
    "# Merge with user_profiles\n",
    "user_profiles = user_profiles.merge(user_city_map, on='deviceId', how='left')\n",
    "\n",
    "# Fill missing with 'UNKNOWN'\n",
    "user_profiles['user_city'] = user_profiles['user_city'].fillna('UNKNOWN')\n",
    "\n",
    "# Check coverage\n",
    "known_users = (user_profiles['user_city'] != 'UNKNOWN').sum()\n",
    "unknown_users = (user_profiles['user_city'] == 'UNKNOWN').sum()\n",
    "\n",
    "print(f\"User location encoding complete:\")\n",
    "print(f\"   Users with known city: {known_users:,} ({known_users/len(user_profiles)*100:.1f}%)\")\n",
    "print(f\"   Users with UNKNOWN city: {unknown_users:,} ({unknown_users/len(user_profiles)*100:.1f}%)\")\n",
    "print(f\"\\nTop 10 user cities:\")\n",
    "print(user_profiles[user_profiles['user_city'] != 'UNKNOWN']['user_city'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Collaborative Filtering Features\n",
    "\n",
    "**Assignment Requirement:** \"Users who read X also read Y\" logic\n",
    "\n",
    "**Optimization:** Compute for users with â‰¥1 interaction (all active users)\n",
    "\n",
    "**Method:** User-user similarity based on reading patterns â†’ neighbor recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1 User-Item Interaction Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building interaction matrix from TRAINING SPLIT ONLY...\n",
      "  Using 2,831,554 events (80% of user history)\n",
      "  Excludes 712,607 validation events (20%)\n",
      "\n",
      "âœ“ Interaction matrix built on training data:\n",
      "  Shape: (8560, 14187)\n",
      "  Users: 8,560\n",
      "  Articles: 14,187\n",
      "  Non-zero entries: 2,207,948\n",
      "  Sparsity: 98.18%\n",
      "\n",
      "ðŸ”’ Data leakage prevented: Matrix built on 80% training events only\n"
     ]
    }
   ],
   "source": [
    "# Use train_split (80% of events) for collaborative filtering\n",
    "print(\"Building interaction matrix from TRAINING SPLIT ONLY...\")\n",
    "print(f\"  Using {len(train_split):,} events (80% of user history)\")\n",
    "print(f\"  Excludes {len(events) - len(train_split):,} validation events (20%)\")\n",
    "\n",
    "# Create ID mappings\n",
    "user_list = train_split['deviceId'].unique()\n",
    "article_list = train_split['hashId'].unique()\n",
    "\n",
    "user_to_idx = {user: idx for idx, user in enumerate(user_list)}\n",
    "article_to_idx = {article: idx for idx, article in enumerate(article_list)}\n",
    "idx_to_user = {idx: user for user, idx in user_to_idx.items()}\n",
    "idx_to_article = {idx: article for article, idx in article_to_idx.items()}\n",
    "\n",
    "# Calculate engagement scores\n",
    "engagement_scores = train_split.copy()\n",
    "engagement_scores['engagement'] = engagement_scores['event_type'].map(event_weights).fillna(1.0)\n",
    "\n",
    "# Aggregate engagement per user-article pair\n",
    "user_article_engagement = engagement_scores.groupby(['deviceId', 'hashId'])['engagement'].sum().reset_index()\n",
    "\n",
    "user_article_engagement['user_idx'] = user_article_engagement['deviceId'].map(user_to_idx)\n",
    "user_article_engagement['article_idx'] = user_article_engagement['hashId'].map(article_to_idx)\n",
    "\n",
    "# Build sparse interaction matrix\n",
    "interaction_matrix = csr_matrix(\n",
    "    (user_article_engagement['engagement'],\n",
    "     (user_article_engagement['user_idx'], user_article_engagement['article_idx'])),\n",
    "    shape=(len(user_list), len(article_list))\n",
    ")\n",
    "\n",
    "sparsity = 1 - (interaction_matrix.nnz / (interaction_matrix.shape[0] * interaction_matrix.shape[1]))\n",
    "\n",
    "print(f\"\\nâœ“ Interaction matrix built on training data:\")\n",
    "print(f\"  Shape: {interaction_matrix.shape}\")\n",
    "print(f\"  Users: {len(user_list):,}\")\n",
    "print(f\"  Articles: {len(article_list):,}\")\n",
    "print(f\"  Non-zero entries: {interaction_matrix.nnz:,}\")\n",
    "print(f\"  Sparsity: {sparsity*100:.2f}%\")\n",
    "print(f\"\\nðŸ”’ Data leakage prevented: Matrix built on 80% training events only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 User-User Similarity Matrix\n",
    "\n",
    "**Engineering Optimization:** Compute for all users with â‰¥1 interaction (covers all active users)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing user similarity from TRAINING SPLIT ONLY...\n",
      "  Total users in training: 8,560\n",
      "  Eligible users (â‰¥10 training interactions): 6,589\n",
      "  Users excluded (<10 interactions): 1,971\n",
      "\n",
      "  Computing user-user similarity matrix...\n",
      "\n",
      "âœ“ User similarity computed:\n",
      "  Users with neighbors: 6,589\n",
      "  Average neighbors per user: 48.7\n",
      "\n",
      "ðŸ”’ Data leakage prevented: Similarities based on training events only\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing user similarity from TRAINING SPLIT ONLY...\")\n",
    "\n",
    "# Count interactions per user in TRAINING data (not all events)\n",
    "user_interaction_counts = train_split.groupby('deviceId').size().to_dict()\n",
    "\n",
    "# Filter users with â‰¥10 interactions in training set\n",
    "eligible_users = [user for user in user_list if user_interaction_counts.get(user, 0) >= 10]\n",
    "eligible_user_indices = [user_to_idx[user] for user in eligible_users]\n",
    "\n",
    "print(f\"  Total users in training: {len(user_list):,}\")\n",
    "print(f\"  Eligible users (â‰¥10 training interactions): {len(eligible_users):,}\")\n",
    "print(f\"  Users excluded (<10 interactions): {len(user_list) - len(eligible_users):,}\")\n",
    "\n",
    "# Extract eligible user rows\n",
    "eligible_matrix = interaction_matrix[eligible_user_indices, :]\n",
    "\n",
    "# Compute user-user similarity\n",
    "print(f\"\\n  Computing user-user similarity matrix...\")\n",
    "user_similarity_matrix = cosine_similarity(eligible_matrix, dense_output=False)\n",
    "\n",
    "# Find top-K neighbors for each user\n",
    "user_similarity_dict = {}\n",
    "K = 50\n",
    "\n",
    "for i, user_id in enumerate(eligible_users):\n",
    "    similarities = user_similarity_matrix[i].toarray().flatten()\n",
    "    similarities[i] = 0  # Remove self-similarity\n",
    "    \n",
    "    top_k_indices = np.argsort(similarities)[-K:][::-1]\n",
    "    top_k_similarities = similarities[top_k_indices]\n",
    "    \n",
    "    # Filter by similarity > 0.1\n",
    "    neighbors = [(eligible_users[idx], sim) for idx, sim in zip(top_k_indices, top_k_similarities) if sim > 0.1]\n",
    "    \n",
    "    user_similarity_dict[user_id] = neighbors\n",
    "\n",
    "print(f\"\\nâœ“ User similarity computed:\")\n",
    "print(f\"  Users with neighbors: {len(user_similarity_dict):,}\")\n",
    "print(f\"  Average neighbors per user: {np.mean([len(v) for v in user_similarity_dict.values()]):.1f}\")\n",
    "print(f\"\\nðŸ”’ Data leakage prevented: Similarities based on training events only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Save All Engineered Features\n",
    "\n",
    "- Metadata: Article features, user profiles\n",
    "\n",
    "**Purpose:** Persist all computed features for use in Algorithms #1, #2, and #3- Collaborative: Interaction matrix, user similarity, ID mappings\n",
    "\n",
    "- Content-Based: TF-IDF matrices, popularity scores, language/type/geo encodings\n",
    "**Files Saved:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features saved successfully!\n",
      "Location: /Users/deepakkumarsingh/Desktop/Inshorts/assignment/data/features\n",
      "\n",
      "Saved features:\n",
      "  - TF-IDF vectorizer and article/user category vectors\n",
      "  - Article popularity scores\n",
      "  - Article locations (inferred from reader geography)\n",
      "  - User-user similarity (â‰¥10 interactions)\n",
      "  - Interaction matrix (sparse)\n",
      "  - ID mappings + newsType encoding\n",
      "  - Article features CSV (9140 articles Ã— 18 columns)\n",
      "    Includes: categories, language, newstype, inferred_location, popularity, language dummies\n",
      "  - User profiles CSV (8689 users Ã— 7 columns)\n"
     ]
    }
   ],
   "source": [
    "features_path = project_root / 'data' / 'features'\n",
    "features_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save TF-IDF features\n",
    "with open(features_path / 'tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "with open(features_path / 'training_tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(training_tfidf, f)\n",
    "\n",
    "with open(features_path / 'testing_tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(testing_tfidf, f)\n",
    "\n",
    "with open(features_path / 'user_category_tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(user_category_tfidf, f)\n",
    "\n",
    "# Save popularity and location features\n",
    "with open(features_path / 'article_popularity.pkl', 'wb') as f:\n",
    "    pickle.dump(article_popularity, f)\n",
    "\n",
    "with open(features_path / 'article_locations.pkl', 'wb') as f:\n",
    "    pickle.dump(article_locations, f)\n",
    "\n",
    "# Save collaborative filtering features\n",
    "with open(features_path / 'user_similarity.pkl', 'wb') as f:\n",
    "    pickle.dump(user_similarity_dict, f)\n",
    "\n",
    "with open(features_path / 'interaction_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(interaction_matrix, f)\n",
    "\n",
    "# Save ID mappings\n",
    "with open(features_path / 'mappings.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'user_to_idx': user_to_idx,\n",
    "        'article_to_idx': article_to_idx,\n",
    "        'idx_to_user': idx_to_user,\n",
    "        'idx_to_article': idx_to_article,\n",
    "        'newstype_mapping': newstype_mapping\n",
    "    }, f)\n",
    "\n",
    "# Add inferred_location column before saving\n",
    "all_content['inferred_location'] = all_content['hashid'].map(article_locations)\n",
    "\n",
    "# Save CSV files\n",
    "all_content.to_csv(features_path / 'article_features.csv', index=False)\n",
    "user_profiles.to_csv(features_path / 'user_profiles.csv', index=False)\n",
    "\n",
    "print(\"All features saved successfully!\")\n",
    "print(f\"Location: {features_path}\")\n",
    "print(f\"\\nSaved features:\")\n",
    "print(\"  - TF-IDF vectorizer and article/user category vectors\")\n",
    "print(\"  - Article popularity scores\")\n",
    "print(\"  - Article locations (inferred from reader geography)\")\n",
    "print(\"  - User-user similarity (â‰¥10 interactions)\")\n",
    "print(\"  - Interaction matrix (sparse)\")\n",
    "print(\"  - ID mappings + newsType encoding\")\n",
    "print(f\"  - Article features CSV ({len(all_content)} articles Ã— {len(all_content.columns)} columns)\")\n",
    "print(f\"    Includes: categories, language, newstype, inferred_location, popularity, language dummies\")\n",
    "print(f\"  - User profiles CSV ({len(user_profiles)} users Ã— {len(user_profiles.columns)} columns)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
